---
title: "Project 1"
author: "Dan Brooks"
date: "June 11, 2017"
output:
  html_document: default
  pdf_document: default
---

#Problem Dsecription

  The purpose of this problem is to create a basic recommender system based off of a series of rankings. A recommender system can be used in a verity of scenarios. It is used by companies like netflix or pandora to recommend movies/songs to people. It is used by amazonr and facebook to recommend advertisments or products to people. 
  
  The dataset that I will be using is radnomly generated by sampling numbers between 1 and 5 (inclusive) and NA values. The rankings of 1-5 simulated what the user randked each item. The nulls represent an item that was not ranked by that particular user. The matrix of ranking values is determined by the input to the function, so it is a n by m ranking system.
  
  The data was separated into two sets. The training and the testing set. The testing set was created by randomly selecting on row from each column in the original dataset. Those valuse and coordinates were saved off into a different dataset and their values in the original were transformed to nulls. That ensures that the testing datset does not have any influence on training the model.
  
  There was two approaches that was utilized to solve this problem. The first was taking the raw average of rankings and using that as the scores. The second was to incorporate some user and item bias into the model. The entire approach was ran multiple times with different rankings each time to see hoe the approaches compare.
  
```{r message=FALSE, echo=FALSE, warning=FALSE}
library(hash)
library(ggplot2)
library(plotly)
```

```{r echo = FALSE}
`%+=%` <- function(e1, e2) eval.parent(substitute(e1 <- e1 + e2))

create_data_set <- function(Rows, Cols)
{
  data <- matrix(NA, nrow = Rows, ncol = Cols)
  responses <- c(1,2,3,4,5,NA)
  
  for (i in 1:Rows)
  {
    for (j in 1:Cols)
    {
      data[i, j] <- sample(responses, 1, replace = TRUE)
    }
  }
  
  return (data)
}

get_test_cases <- function(df)
{
  test_sets <- list()
  
  for (cols in 1:NCOL(df))
  {
    holder <- hash(c("value", "coord"), c(0,0))
    row <- sample(1:NROW(df), 1)
    
    while (is.na(df[row, cols]))
    {
      row <- sample(1:NROW(df), 1)
    }
    
    holder$value <- df[row, cols]
    holder$coord <- c(row, cols)
    
    test_sets[[cols]] <- holder
  }
  
  return (test_sets)
}

test_cases_to_nulls <- function(df, test_cases)
{
  for (values in 1:length(test_cases))
  {
    df[test_cases[[values]]$coord[1], test_cases[[values]]$coord[2]] <- NA
  }
  
  return (df)
}

RMSE_training <- function(df, score)
{
  training_rmse <- 0
  training_count <- 0
  
  for (i in 1:NROW(df))
  {
    for (j in 1:NCOL(df))
    {
      if(is.na(df[i,j]))
      {
        next
      }else
      {
        training_rmse %+=% (df[i,j] - score)^2
        training_count %+=% 1
      }
    }
  }
  
  training_rmse <- sqrt(training_rmse/training_count)
  return (training_rmse)
}

RMSE_test <- function(df, score)
{
  test_rmse <- 0
  test_count <- 0
  
  for (i in 1:length(df))
  {
    test_rmse %+=% (df[[i]]$value - score)^2
    test_count %+=% 1
  }
  
  test_rmse <- sqrt(test_rmse/test_count)
  return (test_rmse)
}

find_biases <- function(df, average)
{
  final_row <- list()
  final_col <- list()
  
  for (i in 1:NCOL(df))
  {
    col_bias <- hash(c("col", "value"), c(0,0))
    
    col_bias$col <- i
    col_bias$value <- mean(df[,i], na.rm = TRUE) - average
    final_col <- c(final_col, col_bias)
  }
  
    for (j in 1:NROW(df))
  {
    row_bias <- hash(c("row", "value"), c(0,0))
    
    row_bias$row <- j
    row_bias$value <- mean(df[j,], na.rm = TRUE) - average
    final_row <- c(final_row, row_bias)
    }
  
  return (list(final_col, final_row))
}

adjust_training <- function(df, average, row_bias, col_bias)
{
  for (rows in 1:length(row_bias))
  {
    for (cols in 1:length(col_bias))
    {
      if (is.na(df[row_bias[[rows]]$row, col_bias[[cols]]$col]))
      {
        next
      }else
      {
        df[row_bias[[rows]]$row, col_bias[[cols]]$col] <- average + row_bias[[rows]]$value + col_bias[[cols]]$value
        
        if (df[row_bias[[rows]]$row, col_bias[[cols]]$col] > 5)
        {
          df[row_bias[[rows]]$row, col_bias[[cols]]$col] <- 5
          
        }else if (df[row_bias[[rows]]$row, col_bias[[cols]]$col] < 1)
        {
          df[row_bias[[rows]]$row, col_bias[[cols]]$col] <- 1
        }
      }
    }
  }
  
  return (df)
}

adjust_test <- function(df, average, row_bias, col_bias)
{
  for (i in 1:length(df))
  {
    df[[i]]$adjusted <- average + row_bias[[df[[i]]$coord[1]]]$value + col_bias[[df[[i]]$coord[2]]]$value
    
    if (df[[i]]$adjusted > 5)
    {
      df[[i]]$adjusted <- 5
      
    }else if (df[[i]]$adjusted < 1)
    {
      df[[i]]$adjusted <- 1
    }
  }
  
  return (df)
}

RMSE_training_adjust <- function(df, adjust_df)
{
  training_rmse <- 0
  training_count <- 0
  
  for (rows in 1:NROW(df))
  {
    for (cols in 1:NCOL(df))
    {
      if (is.na(df[rows,cols]))
      {
        next
      }else
      {
        training_rmse %+=% (df[rows,cols] - adjust_df[rows,cols])^2
        training_count %+=% 1
      }
    }
  }
  
  training_rmse <- sqrt(training_rmse/training_count)
  
  return (training_rmse)
}

RMSE_test_adjust <- function(df)
{
  test_rmse <- 0
  test_count <- 0
  
  for (i in 1:length(df))
  {
    test_rmse %+=% (df[[i]]$value - df[[i]]$adjusted)^2
    test_count %+=% 1
  }
  
  test_rmse <- sqrt(test_rmse/test_count)
  return(test_rmse)
}
```

```{r echo=FALSE}
amount <- 60
training_data <- data.frame(Number = numeric(amount), Type = character(amount), RMSE = double(amount), stringsAsFactors = FALSE)
test_data <- data.frame(Number = numeric(amount), Type = character(amount), RMSE = double(amount), stringsAsFactors = FALSE)
j <- 1
k <- 1

for (i in 1:(amount/2))
{
  overall_df <- create_data_set(6, 8)
  test_cases <- get_test_cases(overall_df)
  training_df <- test_cases_to_nulls(overall_df, test_cases)

  raw_average <- mean(training_df, na.rm = TRUE)
  
  training_RMSE <- RMSE_training(training_df, raw_average)
  test_RMSE <- RMSE_test(test_cases, raw_average)
  
  training_data$Type[j] <- "Training Raw"
  training_data$RMSE[j] <- training_RMSE
  training_data$Number[j] <- k
  test_data$Type[j] <- "Test Raw"
  test_data$RMSE[j] <- test_RMSE
  test_data$Number[j] <- k
  j %+=% 1

  biases <- find_biases(training_df, raw_average)
  row_bias <- biases[[2]]
  col_bias <- biases[[1]]

  adjusted_training <- adjust_training(training_df, raw_average, row_bias, col_bias)
  adjusted_test <- adjust_test(test_cases, raw_average, row_bias, col_bias)

  training_rmse_adjusted <- RMSE_training_adjust(training_df, adjusted_training)
  test_rmse_adjusted <- RMSE_test_adjust(adjusted_test)
  
  training_data$Type[j] <- "Training Bias"
  training_data$RMSE[j] <- training_rmse_adjusted
  training_data$Number[j] <- k
  test_data$Type[j] <- "Test Bias"
  test_data$RMSE[j] <- test_rmse_adjusted
  test_data$Number[j] <- k
  j %+=% 1
  k %+=% 1
}

overall <- rbind(training_data, test_data)
```

#Approach One (Raw Average)

  The raw average approach involves taking all of the rankings inside of teh training set and averaging them all together. THat average will server as the ranking for every item user combo in the training and testing data set. It makes all of the predict rankings the same.
  
#Approach Two (Bias Raw Average)
  
  This apporach will start off the same way as approach number one. The raw average is still calculated the same way as before. The difference is we take that average and add/subtract off bias for the given user and item cobination. The biases are calcuated by taking the everages of each users rating and each items rating. Then thise valuse are subtracted/added to the raw average to get the new rating. This is supposed to help counteract any harsh rankers from one particular user (a low ranking user/item could bring down the overall average fo the sytem).

  
#Graphs
##Comapre Training RMSE

  Below is the graph that compares the training RMSE between the raw average approach and the bias approach. We can see that the RMSE for the bias approach is constanly lower then that of the raw average approach. That does make sense because it is taking into account a lot more then just averaging all the numbers in a set. The little tweaks of the biases appear to help on the training set.
  
```{r}
training_data$Type <- as.factor(training_data$Type)
test_data$Type <- as.factor(test_data$Type)

g <- ggplot(training_data, aes(x = Number, y = RMSE, group = Type, color = Type)) + geom_line()
ggplotly(g)
```

##Compare Test RMSE
  
  Below is the graph comparing the Test RMSE of the bias and RMSE approaches. We can see that this look pretty opposite from the training RMSE's. The raw average appears to be the better option for the testing sets. This could be due to the fact that these are randomly generated values for the training set and testing sets. There really is no rhyme or reason to the rankings that are in the datasets. That means that there really is no "bias" to any of the users or items. 
  
```{r}
p <- ggplot(test_data, aes(x = Number, y = RMSE, group = Type, color = Type)) + geom_line()
ggplotly(p)
```


##Everything together

  Below is the graph of all of the training and testing RMSE's all put together. We can see that the training set for both approaches tend to be lower then the testing sets RMSE. Again, this could be due to the fact that the dataset is randomly generated and there really is no bias amongst the users of the items. The items and users are rated approx. equally across the board, so the raw average approach is the better option.
  
```{r}
h <- ggplot(overall, aes(Number, y = RMSE, group = Type, color = Type)) + geom_line()
ggplotly(h)
```

#Summarize
  
  Overall, for this dataset, the raw average approach was the best. I beloeve that is because the data was randomly generated which gave a relatively even spread amongst all of the ranking. That means that the bias approach was not really able to work the best. The raw average was able to take all of the values and give a nice average of all of them. Kind of what the idea of randomly sampled data is suppsoed to look like.
  
  I personally think the bias approach is a better option for human generated data. I feel that adjusting for low rankers/items is a great idea and can be very powerful for a rather simplistic approach.